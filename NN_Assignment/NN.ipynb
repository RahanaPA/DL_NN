{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5    0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1876b2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the **`sigmoid`** function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
      "       [0.88079708, 0.95257413],\n",
      "       [0.98201379, 0.99330715],\n",
      "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`relu`** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(Z) = (array([[ 1,  3],\n",
      "       [ 0,  0],\n",
      "       [ 0,  7],\n",
      "       [ 9, 18]]), array([[ 1,  3],\n",
      "       [-1, -4],\n",
      "       [-5,  7],\n",
      "       [ 9, 18]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`softmax`** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
      " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
      "[[11 19 10]\n",
      " [12 21 23]]\n"
     ]
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **`initialize_parameters`** function initializes the parameters for all the layers in one `for` loop. \n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
    "\n",
    "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **`sigmoid`**, **`relu`** and **`softmax`** on **Z**.\n",
    "\n",
    "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z =np.dot(W, H_prev) + b #write your code here\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory =sigmoid(Z) #write your code here\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev) + b #write your code here\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)#write your code here\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z =np.dot(W, H_prev) + b  #write your code here\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z) #write your code here\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
       "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
    "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
    "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **`relu`**. The last layer having a different activation **`softmax`** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H #write your code here \n",
    "        \n",
    "        H, memory = layer_forward(H_prev, \n",
    "                                  parameters['W' + str(l)], \n",
    "                                  parameters['b' + str(l)], \n",
    "                                  activation='relu') #write your code here\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H, \n",
    "                              parameters['W' + str(L)], \n",
    "                              parameters['b' + str(L)], \n",
    "                              activation='softmax') #write your code here\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
      " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
      " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
      " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
      " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
      " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
      " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
      " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
      " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
      " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "(784, 10)<br>\n",
    "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
    " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
    " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
    " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
    " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
    " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
    " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
    " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
    " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
    " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n",
    "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **`compute_loss`** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "   # loss =(-1./ m)*average of the sum of all the elements of the matrix Ylog(HL)\n",
    "    loss= (-1. / m) * (np.sum(np.multiply(Y, np.log(HL))))#write your code here, use (1./m) and not (1/m)\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    \n",
    "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
    " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
    " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
    " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
    " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
    " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
    " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
    " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
    " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
    " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
    "[[0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [1. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 1. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 1. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]]<br>\n",
    "0.8964600261334037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
    "\n",
    "### sigmoid-backward\n",
    "You might remember that we had created **`sigmoid`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward\n",
    "You might remember that we had created **`relu`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like **`layer_forward`** calculates **H** using **W**, **H_prev** and **b**, **`layer_backward`** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **`sigmoid_backward`** and **`relu_backward`** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory) #write your code here\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "       \n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) #write your code here, use (1./m) and not (1/m)\n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True) #write your code here, use (1./m) and not (1/m)\n",
    "        dH_prev = np.dot(W.T, dZ) #write your code here\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory) #write your code here\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        \n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) #write your code here, use (1./m) and not (1/m)\n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True) #write your code here, use (1./m) and not (1/m)\n",
    "        dH_prev = np.dot(W.T, dZ) #write your code here\n",
    "        \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "dH_prev is <br>\n",
    " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
    " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
    "dW is <br>\n",
    " [[1.67565336 1.56891359]<br>\n",
    " [1.39137819 1.4143854 ]<br>\n",
    " [1.3597389  1.43013369]]<br>\n",
    "db is <br>\n",
    " [[0.37345476]<br>\n",
    " [0.34414727]<br>\n",
    " [0.29074635]]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **`layer_backward`** here in the loop with the activation function as **`relu`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(W.T, dZ) #write your code here\n",
    "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) #write your code here, use (1./m) and not (1/m)\n",
    "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)  #write your code here, use (1./m) and not (1/m)\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation = 'relu') #write your code here\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp #write your code here\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp #write your code here\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp #write your code here\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
      "   0.        ]\n",
      " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
      "   0.        ]\n",
      " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
      "   0.        ]\n",
      " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
      "   0.        ]\n",
      " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
      "   0.        ]\n",
      " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
      "   0.        ]\n",
      " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
      "   0.        ]\n",
      " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
      "   0.        ]\n",
      " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
      "   0.        ]\n",
      " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
      "   0.        ]]\n",
      "db3 is \n",
      " [[ 0.10031756]\n",
      " [ 0.00460183]\n",
      " [-0.00142942]\n",
      " [-0.0997827 ]\n",
      " [ 0.09872663]\n",
      " [ 0.00536378]\n",
      " [-0.10124784]\n",
      " [-0.00191121]\n",
      " [-0.00359044]\n",
      " [-0.00104818]]\n",
      "dW2 is \n",
      " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n",
      " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n",
      " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n",
      " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n",
      " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n",
      " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "db2 is \n",
      " [[ 0.013979  ]\n",
      " [-0.01329383]\n",
      " [ 0.01275707]\n",
      " [-0.01052957]\n",
      " [ 0.03179224]\n",
      " [-0.00039877]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "\n",
    "dW3 is <br>\n",
    " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
    " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
    " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
    " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
    " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
    " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
    " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
    " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
    " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
    " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
    "db3 is <br>\n",
    " [[ 0.10031756]<br>\n",
    " [ 0.00460183]<br>\n",
    " [-0.00142942]<br>\n",
    " [-0.0997827 ]<br>\n",
    " [ 0.09872663]<br>\n",
    " [ 0.00536378]<br>\n",
    " [-0.10124784]<br>\n",
    " [-0.00191121]<br>\n",
    " [-0.00359044]<br>\n",
    " [-0.00104818]]<br>\n",
    "dW2 is <br>\n",
    " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
    " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
    " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
    " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
    " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
    " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
    " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
    "db2 is <br>\n",
    " [[ 0.013979  ]<br>\n",
    " [-0.01329383]<br>\n",
    " [ 0.01275707]<br>\n",
    " [-0.01052957]<br>\n",
    " [ 0.03179224]<br>\n",
    " [-0.00039877]<br>\n",
    " [ 0.        ]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]#write your code here\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]#write your code here\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n",
    "\n",
    "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have written for feedforward, computing the loss, backpropagation and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions) #write your code here\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters) #write your code here\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y) #write your code here\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories) #write your code here\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate=learning_rate)#write your code here\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.422624\n",
      "Loss after iteration 100: 2.129232\n",
      "Loss after iteration 200: 1.876095\n",
      "Loss after iteration 300: 1.604213\n",
      "Loss after iteration 400: 1.350205\n",
      "Loss after iteration 500: 1.144823\n",
      "Loss after iteration 600: 0.990554\n",
      "Loss after iteration 700: 0.876603\n",
      "Loss after iteration 800: 0.791154\n",
      "Loss after iteration 900: 0.725441\n",
      "Loss after iteration 1000: 0.673485\n",
      "Loss after iteration 1100: 0.631386\n",
      "Loss after iteration 1200: 0.596598\n",
      "Loss after iteration 1300: 0.567342\n",
      "Loss after iteration 1400: 0.542346\n",
      "Loss after iteration 1500: 0.520746\n",
      "Loss after iteration 1600: 0.501865\n",
      "Loss after iteration 1700: 0.485205\n",
      "Loss after iteration 1800: 0.470368\n",
      "Loss after iteration 1900: 0.457054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8774000000000002\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8674000000000002\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
    "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18a6cb38>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3474\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18884c18>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 2486\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18a2a9b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3678\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18a0e0f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3589\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18937278>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 2345\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x189942b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQpUlEQVR4nO3df6zV9X3H8ecLxVgBxy/RKyK2zE2NnbgxR9S0mM7Ogk5ZoinRes3KbhfLdsncD2RZxGXrjOtPXaK9TCogUzqpkU2sMLOKtVkrGooooagDpSD4G1hJKvDeH+d7t8PlnO859/zmfl6P5OSe+31/f7w98rrf7/d8fykiMLOhb1i7GzCz1nDYzRLhsJslwmE3S4TDbpYIh90sEQ77cUrSDyTNbfS0khZK+uca5/sPkubXMu0gl/P7kh5p9nKGGoe9zSRtl/S77e6jX0R8JSIG/UdE0mnAzcC3s99vlHSg6PULSSHpt6qc36WSfiJpv6RNki4v6nE1cKGk3xhsnylz2K1RbgHWRMRBgIhYEREj+1/ArcDrwIuVZiRpLLAa+EdgNHA38G+SxhSN9jDQ09j/hKHNYe9QksZI+ndJb0t6P3t/1oDRpmRrvw8lPZ6FpH/66ZJ+JOkDST+VNKPK5S6S9FD2/mRJD0l6N5vP85JOLzPp54BncmbdDSyL6k7ZvBTYExH/GhGHI+Ih4G3gD4rG+QEwq4p5WcZh71zDgO8Ak4GzgYPAPw0Y52bgD4EzgUPAPQCSJgJPAH8HjAX+HFiVbWoPRjfwK8AkYBzwx1kfpXwS2FqqIGky8ClgWZXLVfYaOOzCot+3AOdIOrXKeSbPYe9QEfFuRKyKiF9ExH7g74FPDxhteURsjoj/Af4GuEHSCcBNFDap10TEkYhYB2wAZg6yjY8ohPxXszXsCxGxr8y4o4H9ZWo3A89GxH9XudwfAWdKmiNpuKRuYApwStE4/csaXeU8k+ewdyhJp0j6tqQdkvYB64HRWZj7vVn0fgcwHBhPYWvg+mzT+wNJHwCXA12DbGM58BTwiKRdku6WNLzMuO8Do8rUbgaWVrvQiHgXuBb4M2APcBXwH8DOotH6l/VBtfNNncPeuW4Dfh34nYg4lcJmMBy9eTup6P3ZFNbE71D4I7A8IkYXvUZExF2DaSAiPoqIOyPiAgr70VdTCG4pm4BfGzhQ0mUUdjMeHeSyn4mI346IscAXKHwWPyka5Xxge86Whg3gsHeG4dmXYf2vEymsuQ4CH2RfvN1RYrqbJF0g6RTgb4FHI+Iw8BBwjaTfk3RCNs8ZJb7gyyXpCkmfzLYm9lH4Y3K4zOhrOHY3Awr7/auyXZHied8iaXvOsi/ONuFPBb4K7IyIp4pG+TTwZPX/Neawd4Y1FILd/1oEfBP4GIU19X8B3y8x3XLgQeAt4GTgTwEi4k0Km8ELKXyL/SbwFwz+//cZFNbI+yh8IfYMhT8kpSwDZkr6WP8ASScDN1B6E34S8FzOsv+S/99K6QJmD6jPITumb9WRb15hjSLpK8DeiPhmFeOuBXojYksNy7kG+EJE3FBDm8ly2M0S4c14s0Q47GaJcNjNEnFiKxcmyV8QmDVZRAw81Rioc80u6SpJWyW9KmlBPfMys+aq+dv47ESLnwFXUjiN8XlgTkS8kjON1+xmTdaMNfslwKsR8XpE/BJ4hMKJHGbWgeoJ+0SOvhBjZzbsKJJ6JG2QtKGOZZlZner5gq7UpsIxm+kR0Qf0gTfjzdqpnjX7To6+6uosYFd97ZhZs9QT9ueBcyV9XNJJwOcp3DfMzDpQzZvxEXFI0jwKNzc4AVgSES83rDMza6iWXgjjfXaz5mvKSTVmdvxw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBE1P58dQNJ2YD9wGDgUEdMa0ZSZNV5dYc9cERHvNGA+ZtZE3ow3S0S9YQ9graQXJPWUGkFSj6QNkjbUuSwzq4MiovaJpTMjYpekCcA64E8iYn3O+LUvzMyqEhEqNbyuNXtE7Mp+7gUeAy6pZ35m1jw1h13SCEmj+t8DnwU2N6oxM2user6NPx14TFL/fP4lIr7fkK7MrOHq2mcf9MK8z27WdE3ZZzez44fDbpYIh90sEQ67WSIcdrNENOJCGDuOnXfeebn1qVOn5tbvueee3Pppp51WtlbpSNCSJUty63Pnzs2t29G8ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGr3hK3bdu23PqUKVNa1MmxDh06lFvv7e3Nrd93332NbOe44avezBLnsJslwmE3S4TDbpYIh90sEQ67WSIcdrNE+Hr2Ie6JJ57IrU+ePLlFnQzeiSfm//M86aSTWtTJ0OA1u1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCB9nH+KmT5+eWz98+HBufd68ebn19evX59YXLlxYtnbTTTflTmuNVXHNLmmJpL2SNhcNGytpnaRt2c8xzW3TzOpVzWb8g8BVA4YtAJ6OiHOBp7PfzayDVQx7RKwH3hsw+FpgafZ+KXBdg/syswardZ/99IjYDRARuyVNKDeipB6gp8blmFmDNP0LuojoA/rAN5w0a6daD73tkdQFkP3c27iWzKwZag37aqA7e98NPN6YdsysWSpuxkt6GJgBjJe0E7gDuAv4rqQvAm8A1zezScuX94z1Std8r127Nrfe19eXWx82LH99MXHixNy6tU7FsEfEnDKlzzS4FzNrIp8ua5YIh90sEQ67WSIcdrNEOOxmifAlrkPA7bffXrY2YsSI3GmvuOKK3HreYT2A2bNn1zX/enTybbA7kdfsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kifJx9CHjjjTdqnnbkyJG59VdeeaXmeTfbjh072t3CccVrdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sET7OPgTcf//9ZWvz58/PnbbS9e42dHjNbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslQhHRuoVJrVuYAbBgwYLc+qxZs3Lr559/fl3zv/POO8vWurq6cqfdunVrbn369Om59Q8//DC3PlRFhEoNr7hml7RE0l5Jm4uGLZL0c0kbs9fMRjZrZo1XzWb8g8BVJYZ/IyKmZq81jW3LzBqtYtgjYj3wXgt6MbMmqucLunmSNmWb+WPKjSSpR9IGSRvqWJaZ1anWsN8HTAGmAruBr5UbMSL6ImJaREyrcVlm1gA1hT0i9kTE4Yg4AiwGLmlsW2bWaDWFXVLxMZPZwOZy45pZZ6h4nF3Sw8AMYDywB7gj+30qEMB24EsRsbviwnyc/bgzYcKE3Hres+EBent7a152d3d3bn358uU1z3soK3ecveLNKyJiTonBD9TdkZm1lE+XNUuEw26WCIfdLBEOu1kiHHazRPhW0pbr0ksvza3PnTu35nmvXr06t75ixYqa523H8prdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEbyWduNGjR+fWn3vuudx6pVtNHzx4sGztsssuy51248aNuXUrreZbSZvZ0OCwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0T4evYhrtKtoDdvzr/l//jx43PrR44cya3feuutZWs+jt5aXrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZomoeJxd0iRgGXAGcAToi4hvSRoLrATOofDY5hsi4v3mtWq1WLx4cW690nH0Sm688cbc+sqVK+uavzVONWv2Q8BtEXE+MB34sqQLgAXA0xFxLvB09ruZdaiKYY+I3RHxYvZ+P7AFmAhcCyzNRlsKXNesJs2sfoPaZ5d0DnAx8GPg9IjYDYU/CED+eZlm1lZVnxsvaSSwCpgfEfukkre5KjVdD9BTW3tm1ihVrdklDacQ9BUR8b1s8B5JXVm9C9hbatqI6IuIaRExrRENm1ltKoZdhVX4A8CWiPh6UWk10J297wYeb3x7ZtYoFW8lLely4FngJQqH3gAWUthv/y5wNvAGcH1EvFdhXr6VdBPce++9ZWt5l5gCvPbaa7n1a665Jre+bdu23HqlS2Ct8crdSrriPntE/BAot4P+mXqaMrPW8Rl0Zolw2M0S4bCbJcJhN0uEw26WCIfdLBG+lXQHGDYs/29ub29vbj3vWPqBAwdyp+3pyT+TeevWrbl1O354zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJaLi9ewNXZivZy/pyiuvzK0/9dRTNc971qxZufUnn3yy5nlbZyp3PbvX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZInw9ewuMGzcut/7oo4/WNf+8+8avW7eurnnb0OE1u1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WiIrH2SVNApYBZ1B4PntfRHxL0iLgj4C3s1EXRsSaZjV6PLv66qtz66NGjcqtL168OLc+f/78srVW3q/AOls1J9UcAm6LiBcljQJekNR/psY3IuKrzWvPzBqlYtgjYjewO3u/X9IWYGKzGzOzxhrUPrukc4CLgR9ng+ZJ2iRpiaQxZabpkbRB0oa6OjWzulQddkkjgVXA/IjYB9wHTAGmUljzf63UdBHRFxHTImJaA/o1sxpVFXZJwykEfUVEfA8gIvZExOGIOAIsBi5pXptmVq+KYZck4AFgS0R8vWh4V9Fos4HNjW/PzBql4q2kJV0OPAu8ROHQG8BCYA6FTfgAtgNfyr7My5tXkseBVq5cmVu/6KKLcuszZszIrb/11luDbcmGsHK3kq7m2/gfAqUm9jF1s+OIz6AzS4TDbpYIh90sEQ67WSIcdrNEOOxmifAjm82GGD+y2SxxDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLRKsf2fwOsKPo9/HZsE7Uqb11al/g3mrVyN4mlyu09KSaYxYubejUe9N1am+d2he4t1q1qjdvxpslwmE3S0S7w97X5uXn6dTeOrUvcG+1aklvbd1nN7PWafea3cxaxGE3S0Rbwi7pKklbJb0qaUE7eihH0nZJL0na2O7n02XP0NsraXPRsLGS1knalv0s+Yy9NvW2SNLPs89uo6SZbeptkqT/lLRF0suSerPhbf3scvpqyefW8n12SScAPwOuBHYCzwNzIuKVljZShqTtwLSIaPsJGJI+BRwAlkXEhdmwu4H3IuKu7A/lmIj4qw7pbRFwoN2P8c6eVtRV/Jhx4DrgFtr42eX0dQMt+NzasWa/BHg1Il6PiF8CjwDXtqGPjhcR64H3Bgy+FliavV9K4R9Ly5XprSNExO6IeDF7vx/of8x4Wz+7nL5aoh1hnwi8WfT7Tjrree8BrJX0gqSedjdTwun9j9nKfk5ocz8DVXyMdysNeMx4x3x2tTz+vF7tCHup+2N10vG/yyLiN4HPAV/ONletOlU9xrtVSjxmvCPU+vjzerUj7DuBSUW/nwXsakMfJUXEruznXuAxOu9R1Hv6n6Cb/dzb5n7+Tyc9xrvUY8bpgM+unY8/b0fYnwfOlfRxSScBnwdWt6GPY0gakX1xgqQRwGfpvEdRrwa6s/fdwONt7OUonfIY73KPGafNn13bH38eES1/ATMpfCP/GvDX7eihTF+fAH6avV5ud2/AwxQ26z6isEX0RWAc8DSwLfs5toN6W07h0d6bKASrq029XU5h13ATsDF7zWz3Z5fTV0s+N58ua5YIn0FnlgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXifwFfRUEP5ShUIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18a74470>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 2594\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18ad2470>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3763\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
